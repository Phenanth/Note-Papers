# Task-Specific Deep Reinforcement Learning

前置知识：

- `imagination`：[文章](https://deepmind.com/blog/article/agents-imagine-and-plan)，[论文](https://arxiv.org/pdf/1707.06203.pdf)
- Intro
  - `Deep Q-Networks`（？）
- 背景
  - `IMPALA`（？）：文中提到它是实现时序训练的训练模型
  - `Huber Loss`：计算误差的函数

概要：

在样本利用率上，以模型为基础的深度增强学习获得了更多的注目。在本文中，我们介绍了一种基于`Imagination-Augmented Agents, I2A`的基于模型的强化学习架构。它能够在降低计算量的同时，使用小量的与特定任务相关的状态表示，来进行未来状态的预估。

更加重要的是，**我们的架构能够使代理能够通过结合当前状态（表示为向量）以及预测状态（编码后的特征值）**，根据任务特性学习恰当的表示方式。由此，代理能够学习如何预测下一个状态的表示，使用按时间顺序排列的训练数据集，共同有效地学习预测状态的方法。

> 和上面那个文章一样的评测游戏，标准是训练时间和执行结果。
>
> 重点应该是标黑体部分的描述，在这里可能会和`I2A`架构在算法上存在不同点。

## 1 Intro

从`Deep Q-Networks`出现开始，深度增强学习就开始在主机游戏上得到运用了。由于主机游戏的方法种类过多（复杂的环境下），纯粹使用没有模型作为基础的增强学习的话，`代理`很难学习到所有可能的状态，因为模型很难在考虑到未来的情况的基础上作出动作。

相比而言，基于模型的增强学习使得代理能够根据环境学习并运用状态转换的模型。运用中，想象的状态转换机制则能够帮助代理在执行动作前找到最合适的动作。使用训练出来的环境模型，代理可以在与环境尽量少互动的情况下学习到一系列有用的方法。

为了解决难以构建完美的复杂环境模型（类似主机游戏，或者是真实世界）问题，类似`I2A`一类的方法运用了神经网络，从预测的多种行动方案（`trajectories`）中提取有用的信息。

基于模型的深度增强学习也存在多个问题。首先，环境中的输入数据往往是高维度的，比如游戏界面，所以环境模型（`environment model`）也必须能够处理高维度输入和输出的问题，这使得学习和测试的时间开销会很大，从而导致学习到的策略的适用性下降。其次，**训练的规划**也很重要。由于代理学习的内容包括许多相互独立的模型，其中包括一个环境模型，运用学习模型的方法，以及对观察结果的表示方法等。较好的训练规划要求包含一个预训练的环境模型以及一个预训练的用于将观测结果转换为低维表示形式的神经网络。如果训练结果比较差，仅仅盲目地运用模型计算出的方法会导致代理无法体验到游戏的后续阶段，从训练结果进行学习的目标也就无从谈起。

我们提出的基于模型的增强学习方法能够使代理从`观察值的低维度的表示形式`结合`向量表示形式的预测状态特征值`的组合向量中，学习到使用模型的方法（`policy`）。训练时，会同步训练`Weighted Actor-Learner Architecture (IMPALA)`这一利用按时间顺序排序的数据集进行训练的模型，共同且有效地通过预测结果计算增强学习的损失函数，从而进行训练。即使在输出是高维度数据的情况下，我们的算法不需要复杂的训练过程也能够在计算成本较小的情况下学习到长期的策略（`strategy`）。游玩推箱子的测试结果可以证明我们的策略能够训练处较好的基于模型的增强学习模型。

## 2 背景

### 2.1 基于模型的增强学习

`MDP, Markov decision processes`已经将环境模型作为增强学习问题中的定式，其中状态转的转换以状态转换函数$T(s_{t+1}|s_t,a_t)$以及反馈函数$r(s_t,a_t,s_{t+1})$表示（$s_t$是时间$t$时刻的状态，$a_t$则是时间$t$时的动作）。

在此计算过程中，我们将考虑确定性环境（`determistic environments`）以及离散动作空间（`discrete action spaces`）。`代理`将以最大化积累反馈$\sum^{无穷大}_{t=0}\gama^tr_t$（其中$\gama$是反馈的折扣参数，$r_t$是`代理`在时间$t$时接收的反馈）为目标进行状态转移参数的学习，并将在后续中用于方法（`policy`）的生成。

### 2.2 I2A

本方法中想到用预测状态来进行方法的学习的思路大大受益于`Imagination-Augmented Agents, I2A`，它是一个能够从不完美的环境模型中学习到预测状态的基于模型的增强学习架构。`I2A代理`能够从当前状态和当前动作（`action`）中学习到环境模型，并将它运用于生成方法。使用`LSTM`卷积神经网络进行进行逆时间序的编码，代理能够从高维度的观察结果中抽取出预测想象结果的特征，从而能够在模型存在不准确性的情况下处理预测状态。

隐藏层中包含普通的无模型代理，它的输出结果会经过多个虚构（`imaginary`）卷展单元进行编码，形成输出，并将用于计算值（`value`）函数和方法函数。

> 省略和`A3C, Asynchronos Advantage Actor-Critic`，一个无模型的学习框架的性能比较。

此外，还会用到推出（`rollout`）方法$\pi^{\^}$用来决定推出的动作是什么。实际上就是输出最接近最终方法函数$\pi$的$\pi^{\^}]$，通过训练从真实动作中提取出未来最有可能做的动作。

此方法的缺点是计算成本太高，因为输入是观察图像（多维）和动作，输出与下一个观测直接对应的图像。在应用到复杂问题上时，维数的升高是个不小的问题。

### 2.3 IMPALA

`IMPALA`是分布增强学习中十分有用的架构，它允许在拥有多个动作者（`actors`）的情况下高效地进行训练。模型的原理是：多个并行运作的动作者（`actors`）决定`IMPALA`模型会使`进行决策时所必须的观察动作`以及`生成的训练数据`分别进行排队，批次地从队伍中创建以及送往学习者（`Learner`）那里去。

由于代理决定动作和学习者用数据进行学习这两个步骤之间有一定时间差，可能会通过其他参与者获取的训练数据进行学习，这就导致行动时方法和学习时方法会有一定偏差（也就是执行的方法有一定延迟）。

为了防止偏差带来执行的误差，不像`A3C`在决定动作时计算值函数，使用动作时计算优势（`advantages`），`IMPALA`观测下一状态的结果是在用数据（如下图）进行训练就给出，优势则是在训练结束后，将观察结果传入值函数网络中进行计算得到。

![1567172605755](C:\Users\SKHR\AppData\Roaming\Typora\typora-user-images\1567172605755.png)

同时，由于代理应当逆序计算$n$步的观察结果，训练数据是按时序排列的。

### 2.4 论文提出的模型

本模型将每个当前状态使用卷积神经网络转化为低维的向量表示形式，并且使用`LSTM`进行下一状态的预测。

预测状态将被另一个`LSTM`编码后，与当前状态拼接并用于计算方法（`policy`）和值函数，这使得代理不仅能训练出满足任务的（向量？）表达方式（`representation`），还能利用预测状态学习如何进行决策（`make a policy`）。

> 个人理解可能是强化了<u>预训练环境模型</u>、<u>预训练的高维转低维卷积神经网络</u>以及<u>核心的决策模型</u>之间的**综合取舍关系**的训练，也就是前面第一章提到的**训练规划**。

关于训练的步骤，简单叙述如下：

1. 将高维的观察结果$o$转化为特征向量$h$
2. 对于每一步行为$a_i$，步骤如下：
   1. 将向量形式的$a_i$输入到一个`LSTM`网络（称为`predictor`，隐藏状态为$h$），预测下一个状态的表示$h_{pred}$，。
   2. 将$h_{pred}$输入到推出（`rollout`）方法$\pi^{\^}$（本质是一个全连接神经网络），预测下一个动作$a_{pred}$
   3. 将$a_{pred}$传回`predictor`，重复步骤1和步骤2$N$次，进入第四步
   4. 将预测结果逆序传给`encoder`（本质是又一个`LSTM`网络），提取出特征$feat_i$
3. 得到所有的特征$feat_n$，将他们与$h$结合，作为前向神经网络的输入，计算方法函数和值函数的结果。

![1567174069480](C:\Users\SKHR\AppData\Roaming\Typora\typora-user-images\1567174069480.png)

由于模型结合了依赖于当前状态的隐藏状态$h$，模型能够逐渐修正前期`predictor`作出的不准确的决策，$h$作为无模型训练方法中的重要参数，帮助模型学习到合理的决策方法。随着学习过程推进，预测器的准确率提高，$h$也成为更好的表达方式（`representation`）。说明，即使仅依赖通过隐藏状态$h$进行状态预测的预测器，也能够学习到更好的决策。

预测器和决策的向量表示都是通过各自的经验进行训练的，这意味着训练过程中，代理需要自己做出决策，并观察这一行为的状态。计算中，我们在预测表达和对应的实际表达之间取`Huber Loss`作为预测误差（依照`Universal Planning Networks. UPN`）。`IMPALA`被用作训练，其输入数据本身就是时序的所以不需要提供额外的信息。由于$h$（预测器的核心产出）对值函数和策略函数都很重要，所以同时训练预测器和策略函数能够省略多余的计算。

防止$h$太容易被预测（`CNN`不能根据预测误差变化），在计算状态预测误差时，我们认为$h$和`CNN`的参数是常数，也不计算`CNN`的梯度。反过来，使用值函数和策略函数来计算增强学习误差时，我们认为动作的向量表示和预测器的参数是常量。

> 个人理解：模型是由两个模型组成的，那么在进行各个模型训练时，会静态化与本模型有关的变量，依赖另一个模型参数的变化进行学习，来加强两个模型之间的相关性。总地来说，就是要同时关注每一步的正确性，也要关注所有步组成的路线的正确性。
>
> 但是我真的不熟悉`I2A`和`LMPALA`...最好弄清$h$，`CNN`的参数，值函数和策略函数，动作向量和预测器的参数都是什么模型里面的...

...

> 这里省略一段提取值函数的公式解释，`pdf p4`，应该是模型的核心（创新部分

由于$h$的初始化值获取问题、还有$h$在一些情况下无法作为隐藏层很好地支持预测器进行下一状态的预测（可能是因为交叉训练，为了获取更合理的策略和值函数参数，而反而变得不适用于本身所处的模型中的计算）等问题，论文为此提出了一种叫`full model`的预测器。使用另一个卷积神经网络从观测结果$o$中获取$h$的初值、或是S预测下一状态时也不强制将$h$作为隐藏层，而是在预测结果出来之后加上一层输出层$P_{out}$。

模型的优点~~（自夸）~~：

1. 使用低精度的环境模型也能处理好从其中获取的预测数据（关于预测器，用`LSTM`来对预测状态编码）
2. 比起直接将图像输入到环境模型中，预先从需要预测的图像中提取低维度的向量表示并将其用作预测，大大减少计算开销（指的是训练步骤一，提取低维表示形式）

与现存架构最为不同的地方在于，我们的模型能够持续高效地，**使用适合（`suitable`）任务的表示形式进行训练**，而且能**良好地应对相对不精确的环境模型**中提取的表示形式，以及解决了**基于学习到的表示形式的环境状态的转换**的问题。

## 3 实践

> 使用方法以及参数的描述。

实验中将提出模型训练的代理和经过无模型训练的代理做对比，测试游戏是推箱子。

大部分的参数设定参照`Racaniere et al.`在2017年进行的实验（预计是`I2A`的）。作为训练数据的是下面这样的图像：

![1567214456961](C:\Users\SKHR\AppData\Roaming\Typora\typora-user-images\1567214456961.png)

还有一些关于代理达成指定条件的得分的描述，见`pdf p4`。

除了`I2A`以外的情况，我们使用一个`CNN`获取512维的$h$隐藏状态，其中激活函数换成一个`Leaky ReLU`。而在无模型方法中，$h$直接被输入到全连接网络，以用于计算值函数和策略。接下来又介绍了动作的向量表示维数为512，输入的长度和预测器和编码器的隐藏层长度也是512，预测步数为5.`IMPALA`则是被用作训练，优势计算的步骤是12，训练的优化器使用`RMSprop`。

为了形成对照，以观察我们的模型有哪些提高，我们还准备了一个虚假预测模型，$$L_{pred}:被设置为0，且从不训练预测器。

> 省略一个统计各个模型的参数数量的表

提出模型能够比无模型方法得分更高（上面省略了得分方法），虚假预测模型则是和无模型方法得分几乎一样，足以体现性能的提高在于预测器而非模型规模。而全模型和简单模型则没有太大区别，原本预计下一个状态预测失败的误差不会影响$h$的表示将会导致准确率下降，虽然在此次实验中并没有体现出来。

比起简单模型，所提出的模型在计算量上是它的三倍。对于I2A，我们测量了在`Racani ere`等人之后使用预训练环境模型训练的情况下的计算时间，和**环境模型的联合学习**作为我们提出的方法。（？）所提出模型比`I2A`训练速度快一个数量级。由于`I2A`使用更多的显存，我们为了在训练次数上能够使提出模型与其平等，还提出了`Proposed(small batch size)`，使用同样批量大小的模型同比训练。注意下面的`I2A`是带环境模型联合学习的训练时间。

> 省略性能对比图，`pdf p5`

## 4 相关研究

### 4.1 使用低维度表示学习的基于模型的增强学习

`RL-HLP,Reinforcement Learning with Hidden Layer Predictor`是运用`A3C`代理的状态转换模型隐藏层中学习到的，表示的未来状态进行方法的学习的。在他们的研究中，使用到了一个预训练的`A3C`代理的`CNN`卷积神经网络，以及前馈神经网络`Softmax(FF())`，并从当前隐藏层和当前动作中训练能够预测下一隐藏状态的网络。但是由于`RL-HLP`网络训练途中，`CNN`和`FF()`是定值，也就是说预训练的`A3C`代理的性能可能会影响到整个网络的性能。

`World Models`则是另一个基于模型的架构。它通过一个变分自动编码器（`Variational Autoencoder`）从环境中获取观测结果。由此，结果的向量表示往往会倾向于包含关于重构图片的特征。比起`RL-HLP`的向量表示中包含的信息是与解决特定任务有关的，它并不依赖于任务的特点，所以能够在即使增强学习的回馈函数发生改变的情况下，也能够再次利用以往的环境模型。然而，由变分自动编码器提取的特征可能并不是某一任务的必需信息，这就导致它的**学习结果很难被特质化到一个具体的任务上**来。训练过程中，为了训练模型的向量表示和状态转换，需要用到随机代理预先收集的观察结果，所以它也**很难即时对游戏后续阶段的变化做出反应**。（`Ha`和`Schmidhuber`则提出了能够使用训练得到的代理生成的观察结果，对变分自动编码器以及状态转换模型重复训练的算法。）

`Buesing et al.`在2018年将各种环境模型于自己的紧凑状态表示模型进行了对比。他们使用了能够将低维输入转化为观察图像的解码器，还训练了能够预测下一个观察状态的环境模型。虽然他们声称自己的模型使用了提出的环境模型以及`I2A`架构，在性能上能够在吃豆人游戏的表现上优于无模型的性能，但是，由于他们在训练代理之前，使用了预训练的方法（`policy`）来形成用于环境模型训练的观察数据，他们的研究结果可能会与`World Model`有同样的弊端。

> 打黑枪时间，总之就是怀疑前人研究内容中使用的预训练内容是否会对其研究成果的性能造成影响。
>
> - `RL-HLP`：`A3C`的`CNN`和`FF()`，用于预测下一隐藏层状态的网络的参数在预训练之后就是定值。
> - `World Models`：
>   - 学习结果难以特定化
>   - 模型的观察结果与模型当前状态之间没有太大关联（因为是由随机代理预收集的数据），导致难以对后续阶段变化（这里暗示包含由于模型做出决策而对环境带来的变化）作出反应。
> - `Buesing`：用来训练环境模型的数据是根据预训练的方法生成的，这就导致用于训练代理的环境模型本身可能就有一定限制（与`World Models`第二点相似）。
>
> 作者自己提到了，在自己的训练中，提出模型比起`I2A`优化了不少，但是全模型（弱化交叉训练对$h$造成的欠拟合？问题的影响）在提出模型基础上的提升并不大。

## 5 总结以及后续工作

此次提出的基于模型的增强学习框架的重点在于：**能够让模型在适合于任务实现的低维度表示的基础上进行下一状态的预测**，这对基于得到的表示的环境状态转换模型、基于获取到状态模型的策略的获取都很有帮助。

以及很重要的一点，即使与复杂的模型结合在一起，我**们的架构也能够帮助压缩预测时间**，将计算时间控制在理想范围内。

未来工作内容：可能会通过结合`MDN-RNN`，来进行随即状态转换的计算。本文中没有提到的使用一个网络来预测奖励的机制也有可能在日后工作中大放光彩。

> 为了让跟随训练不断变化的模型将自己对环境造成的影响也看作是观测数据的一部分，作者引入了`CNN`和`LSTM`网络的交叉训练，弱化模型参数对自身模型的影响，并放大其对另外一个模型的影响。
>
> 但是问题也随之而来，这样又可能造成`LSTM`网络的隐藏层参数$h$对自身模型的拟合不足，导致在`LSTM`在预测下一隐藏状态的阶段的性能就受到折扣，为此作者又提出了改进后的`LSTM`网络并称它为`*full model*`预测器，但是对原始提出模型的提升并不大。
>
> 未来还有可能加入`MDM-RNN`，进行结合随即状态转换的基于模型的增强学习的研究工作。