# 邮件总结顺序

## 研究方向

- 机器学习
- 情报处理
- 自然语言
- 游戏AI

> 好杂

## 学校情况

### 按照教授写

> 每个教授根据自己论文发表的课题写

- 东大
  - 鶴岡 慶雅（已发）
  - 原田 達也
  - 相澤 清晴
  - 山崎 俊彦
  - 宮尾 祐介
- 东工大
  - 岡崎 直観
  - 徳永 健伸
  - 藤井 敦
  - 伊藤 勇太
- 京都大学
  - 黒橋 禎夫
  - 河原 大輔
  - 西野 恒

#### 东大

##### 鶴岡 慶雅

> 已经发过邮件的情况下，直接分类到按照课题里面了

### 按照课题写

- 大阪大学（三个教授属于同一个研究室，且八木教授不会再收了）
- 电气通信大学（末广教授的方向可能不太适合）
- 筑波大学
- 九州大学
- 名古屋大学
- 东北大学（从这里开始）
  - 乾 健太郎（已发）
- 北海道大学

#### 机器学习



#### 情报处理



#### 自然语言处理

##### 使用词预测对运用增强学习的句子生成器加速

> 鶴岡 慶雅：Accelerated Reinforcement Learning for Sentence Generation by Vocabulary Prediction，第一阶段，已发。

在语言处理的领域中，使用神经网络进行句子的生成一直是很重要的内容。作者此番研究的最初目的是通过成熟的`Reinforcement learning`以及`Cross Entrophy`在机器翻译和图像捕捉这两大领域的应用，来对抗地开发自己的训练数据生成器。但最后的落脚点是落在加速后的`Reinforcement Learning`对句子生成的帮助上了，词汇预测器作为点缀出现。

对研究内容进行如下总结：

1. 运用`Reinforcement Learning，下记RL`来进行`Sentence Generation`，这一步骤中包含状态转换和动作选择的迭代。
2. 发现和`Cross Entrophy`训练方法相比，`RL`的训练效率太低。
3. 进一步加入`Vocabulary Prediction`，规定目标句子的长度为K，且以数据集合V作为训练集的基础上训练模型参数。

作为结果来说，这些参数能够用于生成一个较小的单词集V'，以达到加速`RL`训练的作用。

此外，定义强化学习奖励函数的一种简单方法是使用特定于任务的自动评估指标，但这有局限性，因为我们只能将训练数据与目标金句一起使用，这就限制了能够解决的问题的范围。日后，希望研究提出的新方法在综合了增强学习以及`discriminator`的基础上，能够更好地利用生成对抗网络在句子生成这一问题上得到突破，充分发挥生成对抗网络的潜力。

##### 将源语言短语结构纳入神经机器翻译

> 鶴岡 慶雅：Incorporating Source-Side Phrase Structures into Neural Machine Translation，第一阶段，已发。

本文是2016年提出的`tree-to-sequence NMT`在两种方法上的扩展，在`seq2seq`的基础上添加`Source-Side Phrase Structure`，更加关注输入句子的文法结构，在翻译过程中选择目标语言中与源语言定位更加接近的词来进行翻译。

支持运用文法结构进行机器翻译工作的模型原本是`Syntactical Machine Translation, SMT`，但上述的`NMT`模型并不能够借文法结构方法进行词汇选择，于是本文想要提出一种`Syntactical NMT, SNMT`模型，主要使用由底而上的递归方法，借助树形结构的`Recursive Neural Network`来生成句子向量。此外，还在`decoder`中用到了`Attention Mechanism`来有对齐地生成目标词汇。实际实践中，在中译日与英译日的数据上通过与`seq2seq`模型对比的方式进行实践。结果显示文法结构的应用在数据集量小的时候效果明显，但是在使用`bi-directional encoder`的时候效率就不是那么令人满意了。而且训练数据量上升的情况下，文法树也开始失去它的优势。

研究结论包括两点：第一，双向编码器能够同时提高英译日和中译日的准确性和评估算法得分；第二，添加语法树在英译日任务上的性能提高更明显，但是一旦训练数据集的数据量变大，`tree-to-sequence NMT`模型在`WAT 2015 English-to-Japanese`上的提升量开始下降。综上所述，可以从`tree-to-sequence`模型的分析揭示`sequence-to-sequence`模型的不同趋势，还能够明白新提出的模型可以灵活地学习哪些措辞或短语。

#### 游戏AI

##### 在ELF平台的RTS游戏上使用NFSP（`Neural Fictious Self-Play`）

> 鶴岡 慶雅：Neural Fictitious Self-Play on ELF Mini-RTS，第一阶段，已发。

虽然AI在主机游戏上获取了较大成功，但是仍然不能打败`Real-Time Trategy`游戏领域的人类冠军。其中一个原因是`RTS`游戏需要用多代理进行分析，它的环境模型不是一个静态的马尔可夫决策过程（`Markov Decision Process`），也就不能直接套用增强学习的单代理。为此，作者尝试运用`Neural Fictitious Self-Play, NFSP`方法在`ELF`平台上的`Mini-RTS`游戏中进行训练的演绎，以找到满足纳什平衡`Nash Equilibria`的能够在`RTS`这一类游戏中通用的策略。

原本的`Self-Play, FP`训练成果是比较传统的表现形式，每个代理在一局游戏中也只能行动一次，所以它并不适合用在大规模的应用上。为了突破这个限制，新提出了两种方法：`full-width extensive-form FP`以及`Fictious Self-Play, FSP`。两种方法都对表现形式做了一定扩展，前者是一个`full-width`方法，而后者则是较好地经过估算执行的方法（所以在规模变大的时候也能够相应跟随变化）。由于使用了`FP`，`FSP`代理将会不断地游玩一款游戏，将它们的经验存储下来。它们并不直接对全长度的历史对策取平均值，而是利用增强学习来得到一个估计出的最佳平均响应方法。而扩展到`NFSP`后，它不是简单地在`FSP`的基础上应用了`Neural Network, NN`，而是从一个回放池里面取出所有记住的经验（以减少选取采样的等待时间）。`NFSP`还使用了动态预期让每个代理能够有效地跟踪它们的对手行为的变化。

此次研究结果说明了两点：第一，`NFSP`可以和`policy-based reinforcement learning`结合应用，且可以用在`RTS`游戏上；第二，虽然此次的代理都是在无先验知识或者是预先注入了规则的`AI`帮助下完成的训练，但是日后可以用`SF`对`NFSP`预训练以达到提高模型能够处理的样本规模的目的。而对于模型表现并没有想象中好的原因，作者认为此次研究并不满足`Fictious Self-Play, FSP`要求的对过去经验的完美回想条件是比较重要的原因，日后可能会加入`Recurrent Neural Networks, RNN`作为`Reinforcement Learning, RL`组件样本抽取的控制器，并相应地给`Supervised Learning, SL`的储存池的样本保存过程中也加入`Recurrent Neural Networks, RNN`。

##### 运用深度强化学习来学习特定任务的环境模型表示

> 鶴岡 慶雅：Learning Task-Specific Representations of Environment Models in Deep Reinforcement Learning，第一阶段，已发。

在样本利用率上，以模型为基础的深度增强学习获得了更多的注目。在此介绍了一种基于`Imagination-Augmented Agents, I2A`的基于模型的强化学习架构。它能够在降低计算量的同时，使用小量的与特定任务相关的状态表示，来进行未来状态的预估。

本方法中想到用预测状态来进行方法的学习的思路大大受益于`Imagination-Augmented Agents, I2A`，它是一个能够从不完美的环境模型中学习到预测状态的基于模型的增强学习架构。`I2A代理`能够从当前状态和当前动作（`action`）中学习到环境模型，并将它运用于生成方法。而提出的本模型将每个当前状态使用卷积神经网络转化为低维的向量表示形式，并且使用`LSTM`进行下一状态的预测。预测状态将被另一个`Long Short-Term Memory, LSTM`网络编码后，与当前状态拼接并用于计算方法`policy`和值函数，这使得代理不仅能训练出满足特定任务要求的向量形式的状态表达方式，还能利用这样预测到的状态学习到如何进行决策。个人理解，这一步的结果是强化了<u>预训练环境模型</u>、<u>预训练的高维转低维卷积神经网络</u>以及<u>核心的决策模型</u>之间的**综合取舍关系**的训练，也就是在原论文第一章提到的**训练规划**得到了加强。

就结论而言，**提出的架构能够使代理能够通过结合向量形式的当前状态以及编码后的特征值形式的预测状态**，根据任务特性学习恰当的表示方式。由此，代理能够学习如何预测下一个状态的表示，使用按时间顺序排列的训练数据集，共同有效地学习预测状态的方法。除此之外，提出模型虽然比起`I2A`优化了不少，但是全模型可能存在由于弱化了交叉训练对卷积层造成的欠拟合问题的影响，所以在提出模型基础上，提升并不大。

## 总结方法

- 【总】总体的先行研究相关
  - 提出的解决方法
  - 提出方法的研究目的、应用领域
- 【分】为了解决问题详细介绍论文内容
  - 举例关键名词、方法
  - 这些方法解决了什么问题
  - 这些方法解决问题的原理（重点介绍）
  - 这些方法做出的贡献