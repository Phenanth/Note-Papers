# Hybrid MT Based on NMT Model Trained with Phrase Tokens and Large Vocabulary Phrase Translation by SMT
> 这篇是原文日语的

由于神经网络翻译（`Neural Machine Translation, NMT`）存在会因为所知词汇而受到限制的缺点，需要扩大其词汇量才能得到较为准确的翻译结果，在词汇量大幅上升的时候又会成为一个弱点。

> 上面这段没太看懂，需要等会修改

本文希望能够基于神经网络翻译，提出解决大规模词汇问题的解决方式。具体方式：

1. 通过训练用对译文来收集两种语言的词汇之间的对应情报
2. 将对应完毕的对译对子用同一个`Token`编码（替换）
3. 进行`NMT`模型的训练

翻译时，首先用`NMT`模型来对模型中已经有对应的对译集合词汇进行译文生成，其他的部分则会使用传统的基于统计的机器翻译（`Statistic Machine Translation, SMT`）模型进行翻译。

关键词：`NMT`，`SMT`，大词汇，词汇翻译

## 1 Intro

上略一部分关于背景问题，介绍了一些误翻译的内容。

先行研究介绍，目前提出的一些解决方法都是想要扩大`NMT`能够使用的词汇量的规模的：

- 对未知词汇进行分词，试图减少未知词汇量（`[9]~[12]`）
- 将大规模的词汇分割成复数个集合，近似地求取各个词汇的概率（`[2]`）
- 先将未知词汇替换成同意义的已知词汇，再来训练`NMT`模型（`[13]`）
- 导入记录下来的两种语言之间有对应关系的未知词汇`Token`，在翻译的时候将`Token`替换成翻译文（`[4]`）

笔者觉得这些方法最后都会归结为<u>将未知词汇以词汇为单位进行替换</u>这个方法，就会导致很难将复合词翻译到位。

提出方法就是上文概要部分内容，省略。关于性能的总结也省略。

## 2 词汇提取

本章讲述如何基于`branching entropy`这一统计指标，来抽出下文将被转换为`Token`的单词词汇。（这个指标是用来提取关键字词汇，或是分词用的。）具体是使用左边界和右边界的`branching entropy`来判断词汇的边界，并抽出满足条件的词汇。

> 省略`left branching entropy`和`right branching entropy`的定义式。
>
> 关于单词$x$在训练文字中出现的频率按照$f(x)$表示，定义式中的条件概率$P_l$和$P_r$的区别在于根据目标单词$t$和左右集合$v$出现顺序不同。

根据定义式子，如果$t$左右总会出现特定单词，`branching entropy`的值会有变小的倾向。而如果左右出现词的种类和频率不定，它的`branching entropy`的值就会变大。举例而言，对于`bridge`而言，右方固定出现`interface`的几率比较大，所以`bridge`的值会偏小；对于`bridge interface`而言，两方会出现助词，数字，所以`bridge interface`这个词组的值是偏大的。

最后则会提取满足以下条件的词组：

- 整个词组的`branching engropy`高于设定的默认值，而词组的任何子词组低于设定默认值
- 词组不包含记号（此记号集合是人为规定的）
- 设置出现频率由高到低排序的停词集合，词组中不包含出现频率较高的停词

## 3 能够对应大量词组的`NMT`系统

### 3.1 模型训练

训练流程如下：

1. 按后续替换顺序使用`SMT`模型的词汇翻译表以及适用于单词之间对应关系推断的方法确定源语言词汇$t_s^i$对应的目标语言词汇$t_t^i$，抽出词汇翻译对$<t^i_s,t^i_t>(i=1,2,...,k)$
2. 按翻译对出现顺序将其替换成`Token`$<T^1_s,T^1_t>,<T^2_s,T^2_t>,...,<T^k_s,T^k_t>$
3. 使用将翻译对替换为`Token`后的语句进行`NMT`模型的训练

> 这里的`SMT`模型是用的别人的训练结果，见`4.2 小节`，然后我就不懂第二节到底是什么东西了，说不定是`SMT`的参数？有时间再看看`4.3.2 小节`
>
> `Token`是什么形式还是没介绍

### 3.2 使用`NMT`模型进行翻译

翻译的步骤如下：

1. 使用`SMT`模型将原文翻译一遍
2. 从原文和`SMT`的译文中提取翻译对，方法是根据原文提取词汇后，再通过`SMT`的词汇翻译表以及适用于单词之间对应关系推断的方法确定翻译对并提取出$<t^i_s,t^i_t>(i=1,2,...,k)$
3. 根据抽出的翻译对将原文中的对应词汇转换为`Token`
4. 使用`3.1小节`训练的模型，对部分替换完成后的原文进行翻译。这一步后，原本被提取的各个词语的位置对应的就是`NMT`翻译完成后的`Token`所在位置。
5. 根据经过`SMT`提取的翻译对，将`NMT`翻译结果中的`Token`替换为正确的目标语言词汇内容。

> 第二步的描述和`NMT`模型训练过程第一步一模一样
>
> 原理很好懂诶，不愧是学生论文。

> 下面的部分是参数设置，性能评估，关联研究，结束语。
>
> 看心情再总结，有时间再看看`4.3.2 小节`