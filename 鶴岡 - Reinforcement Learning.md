# 鶴岡 - Reinforcement Learning

鶴岡的第一篇论文的阅读笔记，标题：Accelerated Reinforcement Learning for Sentence Generation by Vocabulary Prediction

## 知识点说明

> 复健一下机器学习基础知识

- [Word Embedding](https://blog.csdn.net/L_R_H000/article/details/81320286)，推荐阅读[此文献](https://arxiv.org/pdf/1708.02709.pdf)，下面还有CNN、RecrtNN、RecsvNN等多个近期自然语言处理常用到的模型。
  - n-gram：取一个单词w的前n-1个单词，计算w在上下文中出现的概率，输入是n个词。
  - NLM：取单词法同n-gram，取的语料中任意词w与前n-1个词作为context(w)上下文，每个词作为长度为L的词向量被输入，将context(w)的n-1个词向量首尾拼接为长度为(n-1)*L的长向量输出。使用时，该长向量的第i个分量就可以表示：在训练用到的上下文到基础上，词典中第i个词的概率。相当于是利用了一个词在附近文字段中的贡献大于等于它对全文语义的贡献这一点。
  - Word2vec：在NLM基础上的优化。具体步骤可以见学习笔记`1.6`小节。
    - CBOW：给定上下文基础上预测中心词。
      - 基于`Hierarchical Softmax`，对文本进行编码方式上的改进，输出是一颗[霍夫曼树](https://blog.csdn.net/qq_29721419/article/details/72453518)，其中比较重要的是数值优化的问题，见原文。
      - 基于`Negative Sampling`，将其频数作为随机采样线段的子长度，正负样本标签取自从语料中随机取出的词是否为目标词。对于给定的上下文Context(w)去预测w，如果从语料中就是存在（Context(w),w），那么w就是正样本，其他词就是负样本。构造复杂度小于前者。或者**更直白的表达方式**：计算目标单词和上下文的单词对应的真实单词的“得分”，再加一些“噪声”，即词表中的随机单词和目标单词的对应情况“得分”，来自[知乎说明](https://zhuanlan.zhihu.com/p/29488930)，但是数学原理部分的说法十分模糊，变量说明不清晰。
    - Skip-gram：在有中心词后预测上下文。
- [RNN]([https://zh.wikipedia.org/wiki/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C](https://zh.wikipedia.org/wiki/递归神经网络))
  - [神经网络区别](https://blog.csdn.net/sumresort_lchaowei/article/details/72854389)
    - DNN
    - RNN
    - CNN
- [LSTM](https://zh.wikipedia.org/wiki/%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6)

### Word Embedding

见链接内说明

### DNN

先说DNN，从结构上来说他和传统意义上的NN（神经网络）没什么区别，但是神经网络发展时遇到了一些瓶颈问题。一开始的神经元不能表示异或运算，科学家通过增加网络层数，增加隐藏层可以表达。并发现神经网络的层数直接决定了它对现实的表达能力。但是随着层数的增加会出现局部函数越来越容易出现局部最优解的现象，用数据训练深层网络有时候还不如浅层网络，并会出现梯度消失的问题。我们经常使用sigmoid函数作为神经元的输入输出函数，在BP反向传播梯度时，信号量为1的传到下一层就变成0.25了，到最后面几层基本无法达到调节参数的作用。值得一提的是，最近提出的高速公路网络和深度残差学习避免梯度消失的问题。DNN与NN主要的区别在于把sigmoid函数替换成了ReLU，maxout，克服了梯度消失的问题。下图附有深度网络DNN结构图。

深度学习的深度没有固定的定义，2006年Hinton解决了局部最优解问题，将隐含层发展到7层，这达到了深度学习上所说的真正深度。不同问题的解决所需要的隐含层数自然也是不相同的，一般语音识别4层就可以，而图像识别20层屡见不鲜。但随着层数的增加，又出现了参数爆炸增长的问题。假设输入的图片是1K*1K的图片，隐含层就有1M个节点，会有10^12个权重需要调节，这将容易导致过度拟合和局部最优解问题的出现。为了解决上述问题，提出了CNN。

全连接的DNN还存在着另一个问题——无法对时间序列上的变化进行建模。然而，样本出现的时间顺序对于自然语言处理、语音识别、手写体识别等应用非常重要。对了适应这种需求，就出现了另一种神经网络结构——循环神经网络RNN。

### CNN

CNN最大的利用了图像的局部信息。图像中有固有的局部模式（比如轮廓、边界，人的眼睛、鼻子、嘴等）可以利用，显然应该将图像处理中的概念和神经网络技术相结合，对于CNN来说，并不是所有上下层神经元都能直接相连，而是通过“卷积核”作为中介。同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系。卷积神经网络隐含层，可以用一个例子解释：假设m-1=1是输入层，我们需要识别一幅彩色图像，这幅图像具有四个通道ARGB（透明度和红绿蓝，对应了四幅相同大小的图像），假设卷积核大小为100*100，共使用100个卷积核w1到w100（从直觉来看，每个卷积核应该学习到不同的结构特征）。用w1在ARGB图像上进行卷积操作，可以得到隐含层的第一幅图像；这幅隐含层图像左上角第一个像素是四幅输入图像左上角100*100区域内像素的加权求和，以此类推。同理，算上其他卷积核，隐含层对应100幅“图像”。每幅图像对是对原始图像中不同特征的响应。按照这样的结构继续传递下去。CNN中还有max-pooling等操作进一步提高鲁棒性[^1]。

一个典型的卷积神经网络结构，注意到最后一层实际上是一个全连接层。在这个例子里，我们注意到输入层到隐含层的参数个数瞬间100 * 100，这使得我们能够用已有的训练数据得到良好的模型。适用于图像识别，正是由于模型限制参数了个数并挖掘了局部结构的这个特点，顺着同样的思路，利用语音语谱结构中的局部信息，CNN照样能应用在语音识别中。CNN结构图如下：

[^1]: [计算机科学](https://zh.wikipedia.org/wiki/计算机科学)中，**健壮性**（英语：Robustness）是指一个计算机系统在执行过程中处理错误，以及算法在遭遇输入、运算等异常时继续正常运行的能力。 诸如[模糊测试](https://zh.wikipedia.org/wiki/模糊测试)之类的[形式化方法](https://zh.wikipedia.org/wiki/形式化方法)中，必须通过制造错误的或不可预期的输入来验证程序的健壮性。很多商业产品都可用来测试软件系统的健壮性。健壮性也是[失效评定](https://zh.wikipedia.org/wiki/失效评定)分析中的一个方面。

。

### RNN

对了适应对时间序列上的变化进行建模的需求，就出现了另一种神经网络结构——循环神经网络RNN。在普通的全连接网络或CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被称为前向神经网络(Feed-forward Neural Networks)。而在RNN中，神经元的输出可以在下一个时间戳直接作用到自身，即第i层神经元在m时刻的输入，除了（i-1）层神经元在该时刻的输出外，还包括其自身在（m-1）时刻的输出！表示成图就是这样的：

![image-20190723112635143](/Users/charlotte_chen/Library/Application Support/typora-user-images/image-20190723112635143.png)

我们可以看到在隐含层节点之间增加了互连。为了分析方便，我们常将RNN在时间上进行展开，得到如下图所示的结构：

![image-20190723113102913](/Users/charlotte_chen/Library/Application Support/typora-user-images/image-20190723113102913.png)

RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度！正如我们上面所说，“梯度消失”现象又要出现了，只不过这次发生在时间轴上。对于t时刻来说，它产生的梯度在时间轴上向历史传播几层之后就消失了，根本就无法影响太遥远的过去。因此，之前说“所有历史”共同作用只是理想的情况，在实际中，这种影响也就只能维持若干个时间戳。为了解决时间上的梯度消失，机器学习领域发展出了长短时记忆（LSTM）单元。

### LSTM

见链接内说明

特别注意：作为[非线性](https://zh.wikipedia.org/wiki/非线性)模型，LSTM可作为复杂的非线性单元用于构造更大型[深度神经网络](https://zh.wikipedia.org/wiki/深度学习)。

## 阅读总结

【取自5.2小节】本论文的目的是研究并实现`Reinforcement learning`在句子生成器这一日后将会用到的技术上做优化，其中用到了词汇预测这个方法。

### 2 REINFORCE with Small Vocabularies

#### 2.1 Sentence Generation Models

基于RNN的输出、输入、隐藏状态的核心思想，通过一个一个地输出单词来生成句子，$h_t = RNN(h_{t-1}, e(y_{t-1}), s_{t-1})$，其中$e(y_{t-1})$是输出$y_{t-1}$的word embedding$s_{t-1}$是用于和输入$X$一起翻译的隐藏信息。

这里的任务是：计算生成的词合集$p(y|y<t, X)$，由此预测第$t$个词$y_t$是什么。通常在训练的时候会用到`Gold Sentences`来训练模型，而在测试时，模型处理的是未见过的陌生句子。

##### Machine Translation

基于使用LSTM单元的双向RNN计算每个输入句子$x_i$的隐藏状态$h_i^\sim$（前向与后向的RNN各计算一个d维的隐藏状态$h_i^\to$以及$h_i^\gets$，再将他们连接起来。如果是初始情况的话，$h0 = h_M^\to + h_1^\gets$），再用这个隐藏状态运用[Attention Mechanism](https://www.cnblogs.com/niuxichuan/p/8684807.html)计算：

$s_t = tanh(W_s[h_t;\sum^M_{i=1}a_ih^\sim_i] + b_s)$ 

其中$a_i$是全局的关注函数，$W_s$是权重矩阵，$b_s$是偏重向量。

后续，$s_t$用于计算目标词集合：

$p(y | y < t), X = softmax(W_ps_t + b_p)$  ，其中$W_p$和$b_p$的作用与上面相同。

##### Image Captioning

暂时跳过

$p(y|y<t, X)=softmax(W_ph_t + b_p)$

#### 2.2 Applying Reinforcement Learning

> 前置知识，[Reinforcement Learning](https://blog.csdn.net/coffee_cream/article/details/57085729)

用于弥补SGM在训练与测试时的性能差距`Gap，原文写法`。

这里用到的是最流行的REINFORCE算法，模型设置初始的输入，接下来不断迭代动作选择[^2]以及相应的状态转换[^3]的过程。

在句子生成完毕后，就能够定义相应的估计损失函数，略。另外，不像是最大似然训练，回馈函数应该根据模型的应用领域而有针对性地选择，例如在机器翻译领域能够`BLEU`这样的函数。（本文使用`GLEU`，是`BLEU`在句子层面的变种）

我们定义$R_t=GLEU(Y,Y_g)-b_t$，$b_t$是用于减少梯度在未来时间轴变化幅度的一个基值，是未来的回馈值的一个估计。$b_t$的估计方法是同步地训练一个线性回归模型作为辅助，训练基准是让$||b_t-GLEU(Y,Yg)||^2$尽量小，而它的计算公式为$b_t=\sigma(W_r \cdot s_t+b_r)$，其中$W_r\in R^d$是一个权重向量，$b_r$是偏重，$\sigma(x)$是`Logistic Sigmoid`函数（如果应用场景是图像捕捉，$h_t$会被换成是$s_t$）。

#### 2.3 Large Action-Space Reduction

> 个人体会这部分应该就是体现标题`Accelerated`所在的位置。

训练数据集的组成为来自多个话题，总数大于一万条的句子。然而，在生成一条一百个词组成的句子时，最多只会用到一百个不相同的词。比起输出长度$N$，训练数据数量$|V|$太大了，这就是缩小减小行动区域`Action Space`的初衷。不仅如此，论文作者还发现`REINFORCE`比使用交叉熵损失函数的监督学习慢上好几倍。

为了加速训练，作者提出为每个输入构造一个相较而言比较小的行动区域，也就是构建了一个算法，能够在训练之初首先选出一个比较小的长度为$K$的词集合$V'$。

$V'$的实际表示形式是一个稀疏二元矩阵$M_X \in R^{K \times |V|}$，它使得矩阵在$(i, w_i)$上只有$K$个非零元素。其中$w_i$是$V$中的一个特殊词索引，$M_X$则被用作构造`softmax`层中的参数的子集，公式：$W'_p = M_XW_p, \space\space\space\space\space b'_p = M_Xb_p$。

而这里的参数子集$W'p \in R^{K \times d}$以及$b'_p \in R^K$也将替代原先的参数$W_p$和$b_p$，参与之前提到的迭代计算中的动作选择[^2]的过程。这样就能够完成简化工作。

[^2]: 动作选择对应的是从$p(y | y < t), X = softmax(W_ps_t + b_p)$ 和$p(y|y<t, X)=softmax(W_ph_t + b_p)$中对目标词进行随机取样
[^3]: 状态转换指的是RNN在时序中的迭代$h_t = RNN(h_{t-1}, e(y_{t-1}), s_{t-1})$

### 3 Taraget Vocabulary Prediction

#### 3.1 Input Representations

> 注意控制数据量，避免训练过程在计算资源的损耗上过于昂贵而影响开发迭代的效率。

使用第二节中的模型随机生成数据，为了不影响单词预测模型的训练结果，生成数据的模型参数应当有意地使用不同的参数。

关于参数的相关公式，只介绍与后文相关的内容。输入向量 $v(X) \in R^{d_v}$，该表示方法的计算方法：

- 图像捕捉：$v(X)=W_vf+b_v$
- 机器翻译：$v(X)=\frac1M\sum^M_{i=1}e^\sim_v(|x_i) $，注意$e^\sim_v$与翻译模型的参数$e^\sim$不同

其余内容略。

#### 3.2 Muti-Label Classification

前置知识：[Residual Block](https://zhuanlan.zhihu.com/p/28413039)

计算完$v(X)$后，使用`Residual Block`对该输入进一步转译：$r(X)=Res(v(X)) \in R^{d_v}$，$r(X)$作为实际的输入被输入到预测层 $o=\sigma(W_or(X)+b_o)$，其中$W_o$是权重矩阵，$b_o \in R^{|V|}$是偏重向量。

第$i$个元素$o_i$对应的是在使用给定输入$X$生成的目标句子$Y$所含的目标单词集合$V$中的第$i$个单词。

在这里用于训练单词预测器的数据与句子生成器所用的训练数据相同。对于训练数据中的每条训练数据$X$，都有它的最佳句子`gold target sentence`$Y_g$。

在训练中，我们实际将单词预测器作为一个多标签的分类器训练，它的损失函数为：$-\sum^{|V|}_{i=1}(t_i\log o_i +(1-t_i) \log (1-o_i))$，其中如果$V$中的第$i$个词出现在了$y_g$中，那么$t_i$的值为1，否则为0。（实际情况的话，还使用了`label smoothing technique`，见原文文献）

后面还有关于模型评估方法的描述，总结来说就是在$D$的每个输入$X$的预测层的结果中选取效果最好的$K$个结果，用这些计算一个储存[召回分数](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)的评估矩阵。最后使用评估效果最好的$K$个字为句子生成模型构造一个基于输入内容的单词表$V'$，并严格限制预测模型的召回率[^4]为100%。

[^4]: 召回率：$\frac {TP} {TP + FN}$，代表所有正类别样本中被正确识别为正类别的比例。举例就是实际的垃圾邮件中得到正确分类的电子邮件所占百分比。

#### 后续部分

包括：

- 4 后续实验过程中的参数调整以及引用的方法，比如`Mini-batch splitting`
- 5 实验结果，用于证明在不同的机器学习应用，不同的学习策略下，生成器的结果都令人满意。
  - 5.1 词汇预测的准确度
  - 5.2 句子生成器的准确度
- 6 算法性能评估，对计算速度、GPU内存占用的性能提高，也谈到算法适合在尺寸大一些的`mini-batch size`的情况下用（$S$随之变大）。

不打算重点介绍了，有兴趣的话可以考虑阅读。

## 读后感

作者的最初目的是通过成熟的`Reinforcement learning`以及`Cross Entrophy`在机器翻译和图像捕捉这两大领域的应用，来对抗地开发自己的训练数据生成器。`这里的对抗是指开发过程中使用的训练数据集相同（但是参数不同），模型的数学原理比较相似，优化工作也根据成熟应用的评估结果[^5]对自己的算法进行调整。`

不过文章标题来看，最后的落脚点是落在加速后的`Reinforcement Learning`对句子生成的帮助上了，词汇预测器作为点缀出现。

而如何提到加速`Reinforcement learning`的话，重点应该是`2.3`小节，对训练数据集以及模型表示参数的简化的部分；在评估上，重点是`3.2`这一小节的内容， 主要是关于如何对预测层得到的预测结果做解释[^5]。

总结，标题和论文的解读应该是这样：

1. 运用`Reinforcement Learning，下记RL`来进行`Sentence Generation`，其中包含状态转换和动作选择的迭代。
2. 发现和`Cross Entrophy`训练方法相比，`RL`的训练效率太低。
3. 加入`Vocabulary Prediction`，在句子长度$K$和训练数据集合$V$的基础上训练参数，这些参数将用于生成一个较小的$V'$，这样可以起到对`RL`加速的效果。

> 看下来有种本来是为了开发$A$，结果误打误撞地做起了$B$的优化工作的感觉。
>
> 有些没有细看的位置、没看懂的位置、或者是感觉自己看懂了的位置都需要多看几遍。

[^5]: 存储最高召回值的矩阵，获得的成果就是相应的子单词表$V'$

