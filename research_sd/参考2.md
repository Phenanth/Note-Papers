# 参考2

> 新的两篇用于修改研究计划书的参考文献

##  收集用于语言简易化的词汇集的基于Word Embeddings的词汇对齐方法

> 小町教授做过的内容

### 概要

近几年，人们广泛地研究了使用SMT框架进行句子简化的方法。但是，构架用于句子简化模型训练地单语言词汇集通常要求很高的人工注释成本。所以当前用于句子简化的单语言词汇集仍然被限制在几种语言，比如英语或者葡萄牙语。为了避免人工标注，我们提出了一种基于词嵌入的句子相似度来自动构建单语言平行语料库的非监督方法。对于任何包含复杂句和简单句的句子对，我们采用多对一的方法，将复杂句中的每个单词与简单句中最相似的单词对齐，并通过平均这些单词的相似度来计算句子的相似度。试验结果表明，该方法在单语言平行语料库简化任务中具有良好的性能。结果还表明，使用本文方法建立的语料库训练的SMT框架与使用现有语料库训练的SMT框架相比，在文本简化方面具有更高的准确性。

### 介绍

近年将文本简化视为单语言机器翻译问题，只是需要使用单语言平行语料库进行训练。虽然双语言并行语料库有大量数据可用，但单语言并行语料库很难获得。目前用于文本简化的单语言平行语料库只有7种语言，用于、葡萄牙语、西班牙语、丹麦语、德语、意大利语以及日语。其中只有英语语料库对公众开放。在此，提出一种能够自动建立单语言平行语料库来简化文本，而不使用任何外部资源来计算句子相似度。

本研究中的句子简化单语言平行语料库是建立在一个由简单和复杂文本组成的比较语料库上的。包含两个步骤：

1. 首先，使用词嵌入之间的对齐来计算所有复杂句和简单句的组合（combination）的相似度
2. 第二步，提取相似度超过一定阈值的句子对

后续的评估则包括两部分：

3. 使用平行语料库训练SMT
4. 将复杂句转义成简单句

我们使用一个[基准数据集](http://ssli.ee.washington.edu/tial/projects/simplification)来评估使用我们提出的方法所构建英语文本简化的语料库。基准数据集包括了复杂巨和简单句对子，其二进制标签为parallel或者nonparallel。使用该数据集进行评估的结果（`Intrinsic evaluation`）表明，文章提出的方法具有更好的FI评分。不仅如此，我们还利用结果语料库训练了一个SMT模型，并将它与使用目前语料库进行训练的模型进行了对比。而使用SMT模型进行的句子简化任务评估结果（`Extrinsic evaluation`）表明，提出的方法也有更好的BLEU评分。

文章贡献可以总结如下：

- 单语言平行数据与非平行数据的二分类任务的评分比起之前的研究在F1评分上改进了3.1点（0.607->0.638），并且在构建句子简化平行语料库时展现出了高准确率。
- 使用提出方法构建的平行语料库训练的SMT句子简化模型比使用SOTA单语言平行语料库训练的SMT句子简化模型的BLEU得分提高了3.2分（44.3->47.5）
- 提出方法能够以低成本构建单语言文本简化语料库。因为它不需要其他外界资源，任何比如标记数据或者词典用于计算句子的相似度

### 先行研究

最早是有人做过用英语维基百科和简单英语维基百科进行单语言平行语料库的构建并使用了基于词汇（phrase-based）SMT进行句子简化，并使用了标准的自动MT评估度量BLEU对其进行评估。初次还提出了专门的翻译模型，比如针对短语删除的翻译模型。这些模型都提高了可阅读度以及BLEU得分。本文跟随这些研究使用SMT进行文本简化，使用一种非监督方法同时地提高单语言平行语料库的质与量。

目前基于英语维基百科以及简单英语维基百科构建的其中两种语料库使用的方法并没有考虑单词之间的相似性。在文本模拟中，当计算句子之间的相似性时，考虑同义表达之间的相似性是很有用的，因为概念经常被从复杂的形式改写为简单的形式（这句话的意思是什么？）。剩下一种使用考虑词级相似性来计算句子之间的相似性。我们的方法也考虑了词级相似度来计算句子之间的相似度，但是使用词嵌入来构建一个低成本的文本简化语料库，而不需要访问外部资源。

> 意思是同样都是构建语料库的目的，但是要在研究手法上进行区分。

这些使用英语维基百科以及简单英语维基百科构建的句子简化语料库得到了一些评判的声音。其中有一部分句子并没有对齐，或会导致部分句子变得更加复杂。并有人证明维基百科仍然是提取平行语料库的好来源。Stajner利用SMT框架研究了单语言平行语料库的质量和数量，发现具有中等相似度的句子对训练文本简化模型更加有效。于是，我们使用句子相似方法来度量中等的相似程度。

> 这里的逻辑是先讲述句子简化语料库的先行研究中较多用到的手法是需要计算句子相似度，接下来再讲述如何计算句子相似度

为了解决含有意思相近的不同词语的句子之间的相似度计算问题，人们提出了许多解决方法。在语义文本相似任务中，句子的相似度是通过word2vec等词嵌入完成后，在词相似性的基础上计算出来的。举个例子，在SemEval-2015 Task 2中，使用词嵌入来获得单词对齐的监督方法取得了最好的性能。词嵌入也已经被用于无监督句子相似度的度量。这些无监督句子相似度度量可以应用于自动构建单语言平行语料库以简化文本，不需要对数据进行标记。

> **词嵌入已经是使用过的技术了，那么就要看在NLP领域其他能够用来计算句子相似性的技术是什么，把它替换到词嵌入这里来**

### 基于词嵌入对齐的句子相似度（计算）

一共提出了四种构建单语言平行语料库以简化文本的句子相似度度量方法。3.1-3.3是Song和Roth提出的句子相似度度量方法。3.4讨论了Word Mover's Distance，是另外一种句子相似度度量方法，基于的是在文档分类任务上表现很好的词嵌入的对齐。

> 所以如果在句子相似度方面没有更好的解决方法出现，这个研究就可能没办法进行下去。
>
> 详细暂时省略

#### 平均对齐

> 多对多
>
> 扩展阅读：[欧氏距离和余弦相似度的区别](https://www.zhihu.com/question/19640394/answer/207795500)

提取两个句子中所有的单词对之间的相似度计算：

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200131200959822.png" alt="image-20200131200959822" style="zoom: 67%;" />

使用余弦相似度来计算单词相似度。

#### 最大对齐

> 多对一

但上一节中的方法是不可能有效的，因为即使是同义对，相似度也不会很高。于是在上一节基础上优化为仅仅计算每个单词$x_i$和与其最相似的单词$y_j$来得到句子相似度，而非所有单词之间的匹配性。

为了让相似度得分在计算过程上对称，原计算方法以及改进计算方法如下：

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200131201536253.png" alt="image-20200131201536253" style="zoom: 67%;" />

#### 匈牙利对齐

> 借助图来计算句子相似度（余弦相似度，也可以认为是归一化的欧氏距离）

上方分别可以看作是多对多单词对齐和多对一单词对齐。但未考虑句子级的一致性，为了补足这一点，将两个句子表现为二部图（bipartite graph），其中顶点由每个句子中出现的单词组成，而边缘反映它们的单词级相似性。整个图就可以用作表现句子相似度。整个图是一个加权完全偶图，边缘会被赋予一个词相似（类似上方第一节的余弦相似度）作为权重。**通过寻找二部图的最大匹配，能够得到使单词相似度之和最大的一对一单词对齐**。这个匹配问题可以通过匈牙利算法来解决。然后，通过对每个单词$x_i$使用匈牙利算法选择一个单词$h(x_i)$来计算句子相似度$STS_{hun}$

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200131204558590.png" alt="image-20200131204558590" style="zoom:67%;" />

#### Word Mover距离

这个方法在基于多对多单词对齐上计算句子相似度时同样也考虑了全局的单词对齐一致性。其来源于Earth Mover距离，是一种特殊情况，它解决了将单词从句子$x$传输到句子$y$的运输问题（wut？）。

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200131205300944.png" alt="image-20200131205300944" style="zoom:67%;" />

$\fi(x_u, y_v)$是两个单词的欧氏距离，用于表现两个单词的不相似程度。$A_{uv}$是用于表示$x$句子中的单词$x_u$到句子$y$中的单词$y_v$的转换流的带权矩阵，$n$代表单词数大小，$freq(x_u)$代表$x_u$在句子$x$中的出现频率。

> 按照出现频率对发生转换的单词距离进行带权加和，并用1减去这个和得到句子相似度。

### 在构建句子简化用的单语言平行语料库上的实践

内容：

- 使用这个句子相似度评判方法在句子对的二分类（parallel或nonparallel）的准确性
- 利用提出的句子相似度建立一个用于文本简化的单语言平行语料库，并定性评价
- 使用SMT在语料库和现有的其他语料库上训练文本简化模型，并比较它们的有效性

> 需要详细看看怎么用句子相似度构建语料库的

#### 二分类

利用别人构建的语料库来验证第三节提出的一些句子相似度计算方法，并得出最佳。使用MaxF1和AUC进行二分类的性能。对于不同的对齐评估，令单词相似度大于某个规定值$\theta$来去除词对齐中的噪声。$\theta$根据使MaxF1取值最大进行了相应的调整。

对比通过词嵌入组成的句子嵌入（不包含任何句子对齐）、前人在进行单语言数据集构建时提出的句子相似度测量方法、以及上方提到的另外四种词嵌入方法。利用公开预训练的词嵌入，计算得到表中的句子相似度评分。结论是在G类数据上，Word Mover距离最好；而在G+GP类数据上，最大对齐最好。

> 词嵌入是什么？把[这个](https://blog.csdn.net/u012052268/article/details/77170517)看完
>
> 词向量（词向量）既能够降低维度，又能够capture到当前词在本句子中上下文的信息（表现为前后距离关系），用来表示语言句子词语作为NN的输入
>
> 最好是使用别人预训练的词嵌入作为输入（同语料库），比较方便

由于文本简化不仅需要包含语义（G），也需要删除不重要部分（GP），所以最后将会使用最大对齐。

最后总结了文本简化需要多对一关系的评分，因为短语和单词之间经常会有对应关系。

#### 构建英语简化语料库

我们使用上一步中表现最好的`max alignment`方法，在英文维基百科（普通）和普通英文维基百科上构建单语言平行语料库。首先通过票提的精确匹配将普通版和简单版的文章配对，得到了12w对文章，句子提取使用Wiki Extractor 8，标记（tokenization）则使用NLTK 3.2.1，给出了正常文章的平均单句字数和简单文章的单句字数。分别的平均句子数则是57.7和7.65。

> 首先要思考文本数据从哪来，如果是英文还算好说，中文...恐怕没有这个环境

基于上一部分中的结果（表1），建立了单词相似度和句子相似度的阈值。只对齐那些相似度大于0.49的单词，以及相似度大于0.53的句子。最后获得了49w的句子对。

表2展示了单语言平行语料库中具有句子相似性的句子简化示例。在相似度大于0.9的句子对中发现了同义表达（purchased->bought），在相似度等于或大于0.7的句子对中删除了不是很重要的部分（such as之类的），还发现，只有几个词相似的句子中只有几个词相似。

#### 英文文本简化

我们使用我们的语料库和现有的文本简化语料库训练了SMT文本简化模型，用于对比评价我们的文本简化语料库的有效性。

> 这里已经到评估了，所以估计3才是最重要的部分

### 总结

提出了一种非监督的文本简化单语言平行语料库构建方法。并提出了四种基于词嵌入对齐的句子相似度度量方法。实验结果证明了many-to-one word alignment的句子相似度度量方法在复杂和简单句子中单词的对齐工作的有效性。我们提出的方法在基于英文维基百科中的句子对和简单维基百科中的句子对分类为平行和非平行数据的内在评价和将复杂句子...（外在，和概括一样）

成功地建立了一个英语单语言平行语料库来简化文本。然而，大规模的可比较语料库在很多语言中仍然不能用。未来将会结合我们提出的句子相似度测量和可读性评估进行语料库的进一步构建。特别地说，我们将会将原始语料库基于可读性分解成复杂和简单语料库，并且使用成对的语料库将复杂句与简单对对齐。

## 使用增强学习方法进行句子简化

> 另外一篇，这篇是要有中文的数据做基础，手动收集标注应该也是可以的

### 概要

最近使用机器学习观点来完成单语言从复杂句到简单句的简化改写。文章提出一种基于深度强化学习框架的编码-解码器模型DRESS（**D**eep **RE**inforcement **S**entence **S**implification），探索可能的简化空间，同时学习优化一个奖励函数，鼓励输出简单、有原文意义的输出。在三个数据集上进行验证，具有较好的简化效果。

### 介绍

句子简化任务与NLP应用以及individuals alike方面有关。例如可以用简化组件作为预处理步骤来改进解析器（parser）的性能、总结器（summarizer）以及semantic角色标签器。自动化的简化也有利于低读写能力的人，比如儿童或非母语使用者阅读。

文本简化中最常发生的改写通常是将罕见单词替换成常见单词，简化复杂的语法，删除原始文本的元素等。早期的工作则侧重于简化问题的某些单方面上。例如一些系统，使用句子分割来模拟句法；而其他则是转向词汇的简化，用更常见的WordNet同义词或意译来代替困难的单词。

最近的方法则把简化看作是从SMT中借鉴思想的单语言文本生成任务。改写规则是从英文维基百科和简单英文维基百科中的复杂-简单句子对中自动学习来的。例如，从句法翻译中获得了灵感，并提出了一个模型，附加了针对简化的重写操作（比如分句）。还有人在准同步语法（Quasi-synchronous grammar）框架中提出了简化，并使用整数线性规划对候选翻译/简化进行了评分。

Wubben等则提出了一个两阶段模型：首先，使用复杂-简单句子对进行标准的基于短语的MT（PBMT）模型的训练。在推理过程中，对这个PBMT模型的K-best输出根据它与原先的复杂句子的矛盾性进行重排序。Narayar的混合模型也分为两个阶段。一开始，一个概率模型对Boxer分配的话语表示结构（discourse representation structures）进行句子分割和删除操作。通过类似于Wubben等人的模型，对句子进行进一步的简化。Xu利用简化为目的的特定目标函数和特征，在一个大量平行语法的数据集训练了一个基于语法的MT，以鼓励更简单的输出。

本文则提出一种基于NMT的简化模型。该方法的核心是由递归神经网络实现的编码器译码器结构。编码器将源序列读入连续空间表现形式的列表中，然后解码器从中生成目标序列。虽然我们的模型以编码器-解码器架构为骨干，但它也必须满足简化任务本身的约束，即预测的输出必须更简单，保留输入的意义和语法。为了达到这一共识，模型在强化学习框架中进行训练，它探索了可能的简化空间，同时学习最大化期望的奖励函数，输出达成简化任务所要求的约束条件的结果。强化学习之前已经被应用于提取摘要，信息输出牵引，对话生成，NMT以及图像捕捉。

我们将通过三个开放数据集（自动从维基百科、人写的新闻收集而来）。实验展现强化学习框架是能够成功生成简化文本，并在同样使用这些数据集训练时，能够使其表现能够显著优于强简化模型。

### 神经编码器-解码器模型

我们将首先定义一个文本简化的基础的编码器-解码器模型然后解释如何将它嵌入到强化学习框架中。当给定一个（复杂的）源输入$X=(x_1,x_2,...,x_{|X|})$，模型将会学习简化目标$Y=(y_1, y_2, ..., y_{|Y|})$。通过给定输入预测输出是一个典型的seq2seq学习问题，能够利用编码器-译码器模型建模。句子简化与相关的序列转换任务（例如句子压缩）略有不同，因为它可能会涉及到分割操作。例如长句子可以简化为两个短句子。然而，我们仍然认为目标是一个序列，即将两到三个序列用full stops连接起来。

编码器-解码器模型有两个部分。编码器用LSTM网络将源输入句子转换成一系列的隐藏状态，而解码器使用另外一个LSTM逐个单词地生成简化后的目标输出序列Y。生成动作每次都将依赖之前生成的所有单词和一个动态创建的上下文向量$c_t$，代表的是源句子的编码。

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200117141802616.png" alt="image-20200117141802616" style="zoom: 67%;" />

其中的$g(·)$是一个单隐藏层神经网络，它的参数设置如下

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200117143823162.png" alt="image-20200117143823162" style="zoom:67%;" />

其中$W_o \in R^{|V| \ctimes d}, U_h \in R^{d \ctimes d}$，$|V|$是输出单词长度，而$d$是隐藏层单元数。$h^T_t$是编码器LSTM的隐藏状态，用于概括$y_{1:t}$，即前面所输出的序列：

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200117144352113.png" alt="image-20200117144352113" style="zoom:67%;" />

动态上下文向量$c_t$是输入序列的隐藏状态的带权和：

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200117144343665.png" alt="image-20200117144343665" style="zoom:67%;" />

权重$a_{ti}$是由关注机制所决定的：

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200117144907375.png" alt="image-20200117144907375" style="zoom:67%;" />

两个向量之间使用点乘的原因是要提高计算效率。（提出了很多种，但文章中选了一种）

以上所描述的模型通常以最小化训练用的源-目标数据对的负相关（negative log-likelihood）为训练基准。

### 应用增强学习到句子简化

> 上一部分都是普通的NMT简化模型

这一部分将会提出DRESS模型。尽管序列变化任务中已经有很多成功应用，普通的编码-解码器在句子简化上仍然不能达到最佳效果。使用维基百科数据会让模型关注于将源句子中词汇复制到目标句中，复制来的词汇高达83%。作为结果，通用的编码-解码器会牺牲其他的句子简化重写操作（删除、替换、重排序），让句子实际没有很大的变化。

为了在保持源语句含义和流畅的同时，鼓励更广泛的重写操作，使用了增强学习框架。

1. 编码-解码模型作为读取输入序列X的代理
   1. 代理每一步根据公式(2)的规则，从输出单词集合$V$中选择一个，写作$y^{\^}_t \in V$
   2. 代理继续执行操作，直到产生一个语句结束的标记
2. 输出选择结果序列$Y^{\^}$，也是模型的文本简化任务的输出
3. 得到一个奖励$r$，然后利用REINFORCE算法来更新第一步的代理

![image-20200117162455770](C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200117162455770.png)

接下来介绍奖励以及REINFORCE算法的设计详细内容。

#### 奖励

输出奖励综合概括三个部分：简单性、相关性和流畅性。

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200117164031492.png" alt="image-20200117164031492" style="zoom:67%;" />

$\lambda^S, \lambda^R, \lambda^F$都是属于0-1的小数；$r(Y^{\^})$是$r(X, Y, Y^{\^})$的简写，其中X是源，Y是参考输出，Y^是模型输出。rS, rR, and rF are shorthands for simplicity rS(X, Y, ˆ Y ), relevance rR(X, ˆ Y ), and fluency rF ( ˆY ). 对于每项的介绍总结如下：

**简洁度**

为了鼓励模型运用广泛的简化操作，使用了SARI，一种最近提出的度量方法，用于比较系统输出、引用和输入语句。SARI是添加、复制和删除的n元算术平均值。当系统的输出结果不在引用输出中时，也就是进行了添加操作时，它将会做出鼓励，相似地也会对其他两类进行鼓励。实验证明，它与人类对简单性的判断是相关的，同时也会奖励那些即做出改变又简化输入的模型。

它的缺点在于比较依赖引用输入要包含多个例句，在文本简化中是少见派。虽然Xu为两千个句子提供了八个参考输出，但是它们多用在系统调整而非训练阶段。大多数数据集都只有一个引用输入，且是从英文维基百科中自动对齐生成的。当仅仅依赖单一文献时，SARI会尝试奖励本不应该出现在其中的偶然n-grams（？）。为了支持噪声的影响，以X为源，Y^为系统的输出，Y为参考输出，反过来则是用Y^作为参考，Y作为系统输出。假设系统可以通过交换输出和参考来产生很好的简化结果，反向SARI可以用于估计与系统输出对应的参考对它有多贴合。所以第一个奖励就是SARI和反向SARI的带权和：

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200117224204499.png" alt="image-20200117224204499" style="zoom:67%;" />

**相关性**

当基于简单性的$r^S$试图鼓励模型做出改变时，相关性奖励$r^R$将确保生成的句子保留源句子的含义。我们将使用LSTM语句编码器将源X和预测目标Y转换成两个向量$q_X$和$q_Y^$。相关性奖励就是这两个向量之间的余弦相似度：

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200129110935781.png" alt="image-20200129110935781" style="zoom:50%;" />

再使用一个序列自动编码器SAE对LSTM句子编码器进行复杂句和简单句的训练。特别地，SAE会使用句子$X=(x_1, ..., x_{|X|})$通过一个不包含注意力机制的编码器-解码器来推断自己。首先，编码器LSTM将X转换为一个隐藏状态序列$(h1, ..., h_{|X|})$。然后，使用$h_{|X|}$来初始化解码器LSTM的隐藏状态，每次重新覆盖/生成X个单词。

**流畅度**

Xu发现SARI比起BLEU等其他标准而言，与流畅性等指标的关系更小。流畅度奖励$r^F$使生成的句子更具有明确的格式。它是由经过简单句子训练的LSTM语言模型分配的规格化的句子er概率（normalized sentence probability）：

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200129111800800.png" alt="image-20200129111800800" style="zoom:50%;" />

取Y的perplexity指数来确保$r^F$像前面两者一样都是0-1之间的数值。

#### REINFROCE算法

强化算法的目标是找到一个使期望报酬最大化的代理。每个序列的训练损失是它的负期待反馈：

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200129112106383.png" alt="image-20200129112106383" style="zoom:50%;" />

其中$P_{RL}$是我们的策略，即由解码器模型（公式(2)）得到的分布，而$r(·)$是一个动作序列$Y=(y_1^{\^},..., y^{\^}_{|Y^{\^}|})$，也是生成的简化结果的反馈函数。可惜的是，计算期待项是不可能的，因为可能的动作序列是无穷的。实际上，我们用$P_{LR}(·|X)$的分布的单个样本来近似这个期望。我们参考了Williams对梯度的完整推导。$L(\theta)$的梯度为：

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200129112837738.png" alt="image-20200129112837738" style="zoom:50%;" />

为了减少梯度的方差，我们还引入了一个基线线性回归模型$b_t$来估计$t$时刻的期望未来报酬。$b_t$以$h_t^T$和$c_t$作为输入，并输出一个真实值作为预期的奖励。回归变量的参数使用最小化平防误差训练。在训练过程中，不将错误负反馈给$h^T_t$或者$c_t$。

#### 训练

从最初的形态开始，REINFORCE算法利用随机策略开始学习。但这个策略的假设选择让模型训练在生成任务中变得更具有挑战性，像我们使用大词汇量进行句子生成任务一样。为了解决这个问题，我们通过对代理使用负对数似然对象（第二节）进行预训练，使它能够进行合理的简化，从而能够得到比随机更好的学习策略。此后参照Ranzato的前期成果，采用课程学习策略。在训练初始阶段，我们不给代理全部自主性，只允许它预测每个目标句子的最后几个单词。对于每个目标序列，我们使用负对数似然训练第一个L（最初的时候L=24），并将强化学习算法应用到第L+1个token上。每两个训练epoch，我们设置L=L-3，当L为0时终止训练。

### 词汇的简化

用简单的词替换复杂的词，是句子简化的重要组成部分。该模型能够*联合地*学习词汇替换和其他重写操作。在某些情况下，预测的词语看起来自然，但是作为复杂巨子的内容而言，它们替换得并不巧妙。为此，我们对词汇简化进行了深入的学习，并将其与强化学习模型相结合。

我们使用一个预训练的编码器-解码器模型（该模型用复杂和简单句子的平行语料训练）来获得概率性的词对齐，也就是注意力得分（公式(6)中的$\alpha_t$）。令源句子为$X=(x_1, x_2,..., x_{|X|})$，目标句子为$Y=(y_1, y_2, ..., y_{|Y|})$。使用LSTM将X转换为$|X|$隐藏状态（$v_1, v_2,..., v_{|X|}$）。注意$v_t \in R ^{d \ctimes 1}$对应$x_t$的上下文相关表示。<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200129161245269.png" alt="image-20200129161245269" style="zoom: 50%;" />，给定输入的情况下，输出的每个词的词汇简化概率性以及词对齐分数如下：

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200129161127401.png" alt="image-20200129161127401" style="zoom:50%;" />

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200129161146969.png" alt="image-20200129161146969" style="zoom:50%;" />

词汇简化模型独立地鼓励词汇的替换，不考虑前面已经生成的词汇，所以流畅度是可以保证的（Y？是不是写反了）。简单解决方法是将词汇简化集成到强化学习训练模型中（第三节）中，具体使用线性插入，

<img src="C:\Users\mioto\AppData\Roaming\Typora\typora-user-images\image-20200129161543696.png" alt="image-20200129161543696" style="zoom:50%;" />

### 实践设置

> 还没看

### 结果

回答两个问题：

- 哪种神经模型表现最好
- 资源贫乏且无法获得注释过的训练数据的神经网络模型如何与传统模型/系统竞争

比较了基本的基于注意力机制的编码-解码器模型，DRESS（纯增强学习）模型，以及DRESS-LS（结合词汇替换的增强学习）模型。进一步与两种强基线模型进行比较，分别是PBMT-R以及Hybrid。示例输出结果见Table 3。

神经模型在人工数据上的得分都很好。（省略一大段比较）发现神经模型比比较系统更加流畅，同时执行non-trivial的重写操作（由SARI得分得知）以及获得更简单输出（表2的简单性栏）。还评价了各个模型之间的TER（经过的改写次数、种类）的评分，Hybrid是最高的，但流畅度和句意保留度都有所不足。总的来说，增强学习模型鼓励删除操作，比起EncDecA和PBMT-R也会适当地进行插入操作。

> 下略

### 总结

我们提出的模型可以对输入的简洁性、语法性以及语义忠实性进行联合建模，还提出了一个词汇简化组件，进一步提高整体性能。并发现强化学习能够为结合先验知识到简化任务中提供良好的手段，使模型在三个数据集上的表现得到优化。**将来，可能会对句子拆分和文档简化（非单独句子）进行一些尝试**。除了句子简化外，本文提出的强化学习框架还可以适用于生成任务，比如句子压缩、编程代码生成或作诗等。

## NMT的源语言端单语探索



## 显式语法结构NMT



## 其他参考

- [BLEU](https://www.cnblogs.com/by-dream/p/7679284.html)：主要评判比起参考句子而言，翻译文的对应度和流畅性
  - n-gram匹配度
  - n-gram最大匹配次数
  - 译文长度不可过短

- FKGL：可读性的打分器，分数越低更容易阅读
  - [在线打分器](https://www.prepostseo.com/readability-checker)
- SARI： 一种最近提出的度量方法，它将系统输出与引用和输入语句进行比较