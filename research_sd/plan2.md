# 更新研究计划书

> 目前陷入瓶颈的问题是：
>
> - 在单语言语料库收集方面，很难在小町的基础上进行改进（我不懂）
>
> - 没有很多以中文为研究目标的文本简化成果，目前没找到。就没有办法参考他们的手段对别人的文本简化模型进行修改（我不懂）
>   - 不过小町也说了可以套用别人的英文模型，进行以中文为目的的研究。
>   - 此外，句子简化其实也是一种机器翻译工作，可以在研究中文的时候参考一些涉及中文的机器翻译的文献，利用里面的一些实践方法。
> - 中文没有用作句子简化的语料训练库，需要自己收集
>   - 但是可以考虑用小町的研究方法进行收集，问题就是没有维基和简单维基的类似网站
>
> 
>
> 结论：结合强化学习方法以及【其他中文机器翻译手法】的中文文本（句子或文档）简化实践
>
> - 寻找能够用小町方法进行句子简化任务的中文语料的收集的数据来源，比如网站之类的，并利用小町方法进行数据的收集
> - 利用句子简化的英文模型以及其他涉及中文的的机器翻译成果模型的研究方法/成果，提出自己的研究方法
> - 阅读和以上文献相关的核心文献，完成研究计划书的修改
>
> 
>
> 具体计划：
>
> - 因为只是用作数据的收集，小町的文献方法可以只提到数据收集方法的结论，具体的几种模型之间的区别可以不用分得那么清楚，描写也不需要很清晰
> - 英文的句子简化模型文献以及相关核心文献需要细读
> - 需要寻找以中文为目标的机器翻译模型，简单阅读并概括能够参考的研究手法
>
> 写这个的时候我觉得把一些查过的基础概念写一个合集是很有必要的事情...老年人真是记性不太好。
>
> [纽约时报中文](https://cn.nytimes.com/)
>
> [简单英文维基的对应数据](https://languageresources.github.io/2018/05/17/%E5%8D%A2%E6%A2%A6%E4%BE%9D_%E6%96%87%E6%9C%AC%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE%E9%9B%86/)

## 研究题目

结合强化学习方法以及【其他中文机器翻译手法】的中文文本（句子或文档）简化实践

## 研究背景

> 可以删减
>
> 可能需要添加中文文本简化的背景，最好阅读文献寻找缺少中文语料的相关描述，挑选精确的语言进行这一背景概括

文本简化，也就是使预测的输出必须比输入更简单，并且能够保留输入的意义和语法。最近，常见的用于完成文本简化任务的方法多是基于机器学习方法的。

其中，早期的研究多着眼于替换、语法简化、删除句子成分等单一改写操作的研究~~，例如使用句子分割来模拟句法（这是哪个单词？）的系统，或者是关注词汇的简化。~~

此后则把文本简化任务看作是从SMT中借鉴思想的单语言文本生成任务。其中Wubben等则提出了一个两阶段模型。Wubben等人提出的模型也能够对句子进行进一步的简化。Xu则以文本简化为目的，构建特定目标函数和特征，训练了一个基于语法的MT。

~~本文（参考文献）则提出了一种基于NMT的简化模型。该模型的核心是由递归神经网络实现的编码器-译码器结构。为了更好地进行简化工作，提出模型会进一步地在强化学习框架中进行训练，学习最大化期望的奖励函数。~~

## 研究目的

本研究将会基于强化学习框架训练句子简化模型进行中文文本简化任务的研究。由于中国語のテキスト平易化コーパス的缺少，本文将进行中国語のテキスト平易化コーパス构建工作，得到用于中文文本简化任务的训练语料库。

在此基础上，将会实验实现基于强化学习框架的NMT模型（参考文献），使用收集到的数据集训练该模型，并成功地简化文本，使其在中文语料的表现能够强于其他文本简化模型。

## 先行研究

> 总结单语言语料库构建：
>
> - 单语言语料库构建的先行研究（可以删掉）
> - 句子相似度的计算方法，基于它进行语料的收集
> - 对语料库成果的评估
>
> 总结基于增强学习框架的句子简化模型：
>
> - 神经编码器-解码器模型的构建（简单阐述）
> - 如何应用增强学习框架
>   - 奖励（反馈函数）的设置，包括简洁度，相关性和流畅度
>   - 强化算法的损失函数定义（小节：REINFORCE算法）
> - 优化：结合词汇替换组件（简单阐述）

### 文本简化语料收集

> 说不定需要把这一段全部删掉

近年将文本简化视为单语言机器翻译问题，只是用于训练的单语言平行语料库很难获得。（小町）中提出的单语言平行语料库构建手法包含两个步骤：首先，使用词嵌入之间的对齐方法来计算所有复杂句和简单句的组合（combination）的相似度；其次，提取相似度超过一定阈值的句子对。

为了选择性能最优的词嵌入对齐方法用于构建语料库，作者对比了四种句子相似度度量方法~~（Average Alignment、Maximum Alignment、Hungarian Alignment以及Word Mover’s Distance）~~在已存在语料库（Hwang et al. (2015）上的二分类效果。最后，作者使用性能最佳的句子对齐方法Maxinum Alignment~~（由于文本简化不仅需要保留句义，也需要删除句子的不重要部分，所以最佳选择是Maximum Alignment）~~构建了文本简化语料库。

在此之后，使用此语料库训练了SMT，并将其与使用现存的其他语料库训练的SMT进行了性能对比。结果证明，其构建的语料库训练的SMT在文本简化结果上具有更高的准确性。

### 文本简化模型训练

文章提出一种基于深度强化学习框架的编码-解码器模型DRESS（**D**eep **RE**inforcement **S**entence **S**implification），其特点是在~~用于使文本简化的基础编码器-解码器模型在~~保持源语句含义和流畅的同时，鼓励更多样的文本重写（比如删除、替换、重排序）操作。该增强学习框架的设计内容包括奖励函数的设置以及结合REINFORCE算法。

其中，奖励综合包含简单性、相关性和流畅性的三个部分。它们分别保证模型做到：在句子生成过程中会运用广泛的简化操作、保留源句子含义、其生成的句子具有更明确的格式。

而REINFORCE算法的目标是找到一个使期望报酬最大化的Agent。作者在训练过程中使用了Williams（需要插入参考）对梯度的完整推导来简化期待项的计算，并参照了Ranzato的前期成果，采用课程学习策略来得到更好的训练结果。

将提出模型以及现存的文本简化模型比较得出，神经网络模型总体而言比比较模型性能更佳，具体体现在前者对句子的改写操作更多，人工评价的句子简单性也更好。

## 研究方法

### 单语言数据获得

> 最好能找到现成的基于维基百科的英-中词嵌入分布，不行的话需要找**英-中平行语料库构建方法**
>
> 找到了

1. 利用（小町方法），构建文本简化的英文单语言语料库；或者使用现有的同样基于维基百科的[英文文本简化数据集](https://languageresources.github.io/2018/05/17/%E5%8D%A2%E6%A2%A6%E4%BE%9D_%E6%96%87%E6%9C%AC%E7%AE%80%E5%8C%96%E6%95%B0%E6%8D%AE%E9%9B%86/)
2. 利用Lu提出的英-中平行语料库构建方法，构建基于维基百科词条的英-中平行语料库
3. 将英文单语言语料库中的简单句翻译成中文，将其视为中文简单句，并和视为中文复杂句的平行语料库中的中文两两组合，组合的结果视作一条平行语料。（删除两边语料中无对应的句子）
4. 使用（小町方法）等对齐方法对构建的中文简化语料的相似度进行评估

### 文本简化实践

> 中文的文本简化工作开展并不多，而考虑到文本简化也是句子生成/机器翻译，所以应该参考**目标语言是中文的机器翻译研究成果**，并参考其中的实践手法。

1. 参考[中文词向量](https://github.com/Embedding/Chinese-Word-Vectors)项目提供的中文维基百科词向量开放数据，对平行语料库的数据进行预处理，得到适合模型训练的词向量表示
2. 在构建的平行语料词向量的基础上训练（DRESS）提出的运用REINFORCE算法的编码-解码器模型，并用BLEU等方法对文本简化结果进行评估

## 参考文献

